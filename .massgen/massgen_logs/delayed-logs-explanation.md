Excellent! Now let me analyze the logs to identify the performance bottleneck.







# Root Cause Found: Excessive Agent Restarts

Based on the logs, your session took **22.3 minutes** instead of the expected time due to **17 agent restarts**.

## Timeline Breakdown

```yaml
Start: 22:55:13
End:   23:08:31
Total Duration: 1,337 seconds (22.3 minutes)
```

## The Problem: Restart Cascade

From the coordination table summary:

```@d:\MassGen\.massgen\massgen_logs\log_20251025_225510\coordination_table.txt#225:230
| Answers  |         1 answer        |         1 answer        |         1 answer        |         1 answer        |         1 answer        |         1 answer        |         1 answer        |
| Votes    |          1 vote         |          1 vote         |          1 vote         |          1 vote         |          1 vote         |          1 vote         |          1 vote         |
| Restarts |        3 restarts       |        3 restarts       |        2 restarts       |        3 restarts       |        3 restarts       |        1 restart        |        2 restarts       |
| Status   |       ‚úÖ Completed       |       ‚úÖ Completed       |       ‚úÖ Completed       |       ‚úÖ Completed       |       ‚úÖ Completed       |       ‚úÖ Completed       |       ‚úÖ Completed       |
|----------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+
| TOTALS   |                                                                           7 answers, 7 votes, 17 restarts                                                                           |
```

### What Happened:

1. **All 7 agents started streaming simultaneously** (E1-E7)
2. **Agent 5 provided first answer** ‚Üí triggered restart for all agents
3. **Restart Cascade**: Each agent that answered triggered a restart affecting all 7 agents:
   - Agent 5 ‚Üí restart #1
   - Agent 1 ‚Üí restart #2
   - Agent 4 ‚Üí restart #3
   - Agent 2 ‚Üí restart #4
   - Agent 3 ‚Üí restart #5
   - Agent 7 ‚Üí restart #6
   - Agent 6 ‚Üí restart #7
   - Then more restarts as agents retried...

4. **17 total restarts** = 17 √ó 7 agents = **119 agent API calls just for restarts**

Let me check if there are rate limit errors:



# Performance Issue Analysis: 22.3 Minutes Execution

## Summary of Issues

**Duration**: 1,337 seconds (22.3 minutes)  
**Expected**: ~2-3 minutes  
**Efficiency**: ~10x slower than expected

---

## Root Causes

### 1. **Excessive Restart Cascades (Primary Issue)**

**17 total restarts** triggered a multiplicative effect:
- Each restart affects **all 7 agents**
- 17 restarts √ó 7 agents = **119 extra agent stream operations**

**Restart Timeline:**
```
Agent 5: 2.78s  ‚Üí restart all
Agent 1: 2.84s  ‚Üí restart all (0.06s later)
Agent 4: 3.62s  ‚Üí restart all
Agent 2: 4.41s  ‚Üí restart all
[56 SECOND GAP - Rate limiting suspected]
Agent 3: 60.19s ‚Üí restart all
Agent 7: 60.48s ‚Üí restart all  
Agent 6: 60.55s ‚Üí restart all
```

### 2. **56-Second Bottleneck** 

Between restart #4 and #5, there's a **56-second gap** where agents were stuck streaming. This suggests:
- **Gemini API rate limiting** (429 errors or throttling)
- Slow responses from `gemini-2.5-pro` models (agents 6 & 7)
- Sequential retry backoffs

### 3. **Why Restarts Are Happening**

Need to check why agents trigger restarts. Let me examine the MassGen orchestration logic:





# **ROOT CAUSE IDENTIFIED**

## The Issue: Design Pattern + API Rate Limiting

### 1. **Restart-on-Answer Design Pattern**

From the orchestrator code:

```python
# Line 608-609 in orchestrator.py
restart_triggered_id = agent_id  # Last agent to provide new answer
reset_signal = True
```

**Every time ANY agent provides a new answer, ALL agents restart**. This is intentional design to keep agents synchronized, but it creates a multiplicative problem:

- **7 agents** provide initial answers ‚Üí **7 restarts** √ó **7 agents** = **49 API calls**
- Plus the original 7 streams = **56 API calls in rapid succession**
- Additional restarts for re-answers add more

### 2. **Gemini API Rate Limiting (56-Second Gap)**

The **56-second pause** between restarts #4 and #5 indicates:
- Gemini API hit rate limits after ~20-30 rapid requests
- System had to wait for rate limit window to reset
- This multiplied across all 17 restarts

### 3. **Configuration Has No Rate Limit Controls**

```yaml
orchestrator:
  max_duration: 600
  consensus_threshold: 0.6
  max_debate_rounds: 2
```

No settings for:
- Request throttling
- Concurrent agent limits
- Retry backoff strategies

---

## Recommendations to Fix

### Immediate Actions:

**1. Reduce Agent Count for Testing**
```yaml
# Use 2-3 agents instead of 7
agents:
  - id: "agent_1"
    backend: {type: "gemini", model: "gemini-2.5-flash"}
  - id: "agent_2"
    backend: {type: "gemini", model: "gemini-2.5-pro"}
```

**2. Add Rate Limiting Configuration**
Check if MassGen supports rate limit settings:

Perfect! MassGen has a rate limiter. Let me examine it:



# **COMPLETE ROOT CAUSE ANALYSIS**

## Why 22.3 Minutes Instead of 2-3 Minutes

### The Perfect Storm of Rate Limiting

Your config uses **7 concurrent Gemini agents**:
- **5√ó gemini-2.5-flash** agents (limit: **9 req/min** shared)
- **2√ó gemini-2.5-pro** agents (limit: **2 req/min** shared)

From the Gemini backend code:

```python
# Lines 145-158 in massgen/backend/gemini.py
if 'gemini-2.5-flash' in model_name.lower():
    self.rate_limiter = GlobalRateLimiter.get_limiter_sync(
        provider='gemini-2.5-flash',
        max_requests=9,  # 9 requests per minute
        time_window=60
    )
elif 'gemini-2.5-pro' in model_name.lower():
    self.rate_limiter = GlobalRateLimiter.get_limiter_sync(
        provider='gemini-2.5-pro',
        max_requests=2,  # Only 2 requests per minute!
        time_window=60
    )
```

### The Math That Killed Performance

**Initial Phase (all agents start):**
- 7 agents start streaming = **7 API calls**
- Agent 5 finishes first ‚Üí triggers restart for all 7 = **+7 calls**
- Agent 1 finishes ‚Üí restart all 7 = **+7 calls**
- Agent 4 finishes ‚Üí restart all 7 = **+7 calls**
- Agent 2 finishes ‚Üí restart all 7 = **+7 calls**

**After 4 answers: 35 API calls needed**

**Flash model budget: 9 calls/minute**
- 35 calls √∑ 9 per minute = **~4 minutes just waiting**

**Pro model budget: 2 calls/minute** 
- 2 pro agents √ó multiple restarts = **severe bottleneck**
- This explains the **56-second gap** before agents 3, 6, 7 could proceed

**Total with 17 restarts:**
- 17 restarts √ó 7 agents = **119 restart API calls**
- Plus 7 initial streams = **126 total API calls**
- Flash: 126 √∑ 9 = **14 minutes**
- Pro: Even slower due to 2/min limit

---

## Solutions

### **Quick Fix: Reduce Agent Count**

```yaml
# test_3_gemini_agents.yaml
agents:
  - id: "agent_1"
    backend: {type: "gemini", model: "gemini-2.5-flash", temperature: 0.7}
  - id: "agent_2"
    backend: {type: "gemini", model: "gemini-2.5-flash", temperature: 0.8}
  - id: "agent_3"
    backend: {type: "gemini", model: "gemini-2.5-pro", temperature: 0.7}

orchestrator:
  max_duration: 180
  consensus_threshold: 0.6
  max_debate_rounds: 1  # Reduce restarts
```

**Expected improvement:**
- 3 agents √ó 3 restarts = 9 API calls
- 9 √∑ 9 per minute = **~1 minute** (10x faster!)

### **Medium-term: Use Higher-Tier API Keys**

Gemini free tier limits are restrictive. Consider:
- **Paid tier**: 300-1000 RPM instead of 2-10 RPM
- **Check your API quota**: https://aistudio.google.com/app/apikey

### **Long-term: Optimize Restart Logic**

Consider modifying MassGen to:
1. Only restart agents that haven't answered yet
2. Add `sequential_mode` to process agents one-by-one
3. Implement answer caching to avoid redundant restarts

---

## Summary

**Your session hit Gemini's strict rate limits:**
- 7 agents √ó restart-on-every-answer = 100+ API calls
- 9 calls/min for Flash + 2 calls/min for Pro = severe throttling
- Result: 22.3 minutes of mostly waiting

**Action**: Use 2-3 agents max with free-tier Gemini, or upgrade to paid tier for 7+ agents.


---------------------------------------------------------




I'll trace through the code to show you exactly where and why this happens.



# Restart Mechanism Explained: Where and Why

## The Complete Flow

### 1. **Agent Provides Answer ‚Üí Triggers Restart**

When ANY agent provides a new answer, the orchestrator immediately sets a restart signal:

```@d:\MassGen\massgen\orchestrator.py#586:609
if result_type == "answer":
    # Agent provided an answer (initial or improved)
    agent = self.agents.get(agent_id)
    # ... (save snapshot, etc)
    
    answered_agents[agent_id] = result_data
    # Pass timestamp to coordination_tracker for mapping
    self.coordination_tracker.add_agent_answer(
        agent_id,
        result_data,
        snapshot_timestamp=answer_timestamp,
    )
    restart_triggered_id = agent_id  # Last agent to provide new answer
    reset_signal = True  # ‚Üê THIS TRIGGERS THE RESTART CASCADE
```

### 2. **Restart Signal Marks ALL Agents for Restart**

After processing the answer, the orchestrator marks **every agent** with `restart_pending = True`:

```@d:\MassGen\massgen\orchestrator.py#740:752
# Apply all state changes atomically after processing all results
if reset_signal:
    # Reset all agents' has_voted to False (any new answer invalidates all votes)
    for state in self.agent_states.values():
        state.has_voted = False
    votes.clear()

    for agent_id in self.agent_states.keys():
        self.agent_states[agent_id].restart_pending = True  # ‚Üê ALL agents marked

    # Track restart signals
    self.coordination_tracker.track_restart_signal(restart_triggered_id, list(self.agent_states.keys()))
```

### 3. **Each Agent Checks restart_pending and Exits Early**

Inside each agent's execution loop, the orchestrator checks the flag and gracefully terminates:

```@d:\MassGen\massgen\orchestrator.py#1564:1577
for attempt in range(max_attempts):
    logger.info(f"[Orchestrator] Agent {agent_id} attempt {attempt + 1}/{max_attempts}")

    if self._check_restart_pending(agent_id):
        logger.info(f"[Orchestrator] Agent {agent_id} restarting due to restart_pending flag")
        # Save any partial work before restarting
        await self._save_partial_work_on_restart(agent_id)
        yield (
            "content",
            f"üîÅ [{agent_id}] gracefully restarting due to new answer detected\n",
        )
        yield ("done", None)  # ‚Üê Agent terminates gracefully
        return  # ‚Üê Exit execution
```

### 4. **Main Loop Restarts Agents Automatically**

The main coordination loop continuously checks which agents aren't running and restarts them:

```@d:\MassGen\massgen\orchestrator.py#506:525
# Stream agent outputs in real-time until all have voted
while not all(state.has_voted for state in self.agent_states.values()):
    # Start new coordination iteration
    self.coordination_tracker.start_new_iteration()
    
    # Start any agents that aren't running and haven't voted yet
    current_answers = {aid: state.answer for aid, state in self.agent_states.items() if state.answer}
    for agent_id in self.agents.keys():
        if agent_id not in active_streams and not self.agent_states[agent_id].has_voted:
            # Apply rate limiting before starting agent
            await self._apply_agent_startup_rate_limit(agent_id)
            
            active_streams[agent_id] = self._stream_agent_execution(  # ‚Üê Agent restarted
                agent_id,
                self.current_task,
                current_answers,  # ‚Üê NOW includes the new answer
                conversation_context,
            )
```

### 5. **restart_pending Flag Cleared at Start**

When an agent restarts, the flag is cleared so it can begin fresh:

```@d:\MassGen\massgen\orchestrator.py#1411:1416
# Clear restart pending flag at the beginning of agent execution
if self.agent_states[agent_id].restart_pending:
    # Track restart_pending transition (True ‚Üí False) - restart processed
    self.coordination_tracker.complete_agent_restart(agent_id)

self.agent_states[agent_id].restart_pending = False  # ‚Üê Flag cleared
```

---

## Why This Design?

The comments in the code explain the reasoning:

```@d:\MassGen\massgen\orchestrator.py#494:496
Restart Mechanism:
When any agent provides new_answer, all other agents get restart_pending=True
and gracefully terminate their current work before restarting.
```

### The Rationale:

**1. Keep Agents Synchronized**
- When Agent A discovers a new insight, other agents need to see it
- Prevents agents from working with stale information
- Ensures all agents vote on the **same set of answers**

**2. Invalidate Old Work**
- Any votes in progress become meaningless once a new answer appears
- Any answers being written might be redundant
- Clean slate ensures fresh perspective on the new answer

**3. Context Sharing**
```@d:\MassGen\massgen\orchestrator.py#740:744
if reset_signal:
    # Reset all agents' has_voted to False (any new answer invalidates all votes)
    for state in self.agent_states.values():
        state.has_voted = False
    votes.clear()  # ‚Üê All votes invalidated
```

---

## The Problem

This design creates a **multiplicative effect**:

- 7 agents each provide 1 answer = 7 answers
- Each answer triggers restart for **all 7 agents**
- 7 answers √ó 7 restarts = **49 restart operations**
- 49 restarts √ó API calls = severe rate limiting

### Why Not Optimize?

The current implementation prioritizes:
1. **Correctness** over performance (agents always see latest state)
2. **Simplicity** (easy to reason about: new answer = everyone restarts)
3. **Collaborative accuracy** (all agents vote with complete information)

But this trades off **speed**, especially with:
- Many agents (7+)
- Rate-limited APIs (Gemini free tier)
- Multiple debate rounds

---

## Potential Optimizations (Not Implemented)

**Option 1: Selective Restart** - Only restart agents that haven't answered yet  
**Option 2: Batch Answers** - Collect answers for X seconds before restarting  
**Option 3: Sequential Mode** - Process agents one-by-one (no restarts needed)  

These would require significant architectural changes to the orchestrator.